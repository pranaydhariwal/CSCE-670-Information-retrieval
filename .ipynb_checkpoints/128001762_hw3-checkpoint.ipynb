{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2019\n",
    "\n",
    "\n",
    "# Homework 3:   Collaborative Filtering + Matrix Factorization\n",
    "\n",
    "### 100 points [6% of your final grade]\n",
    "\n",
    "### Due: Wednesday, March 27, 2019 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Understand how collaborative filtering and matrix factorization works. Explore different methods for real-world recommendation senarios.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw3.ipynb`. For example, my homework submission would be something like `555001234_hw3.ipynb`. Submit this notebook via eCampus (look for the homework 3 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Collaborative Filtering (50 points)\n",
    "\n",
    "In this part, you will implement collaborative filtering on the [MovieLens Latest Datasets](http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html). After removing users who left less than 20 ratings and movies with less than 20 ratings, the provided dataset has only ~1,200 items and ~500 users. You can also check the title and genres of each movie in *movies_info.csv*.\n",
    "\n",
    "As background, read [Collaborative Filtering Recommender Systems](http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf) and refer to the course slides `week06rec.pdf` for collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load the Data\n",
    "\n",
    "Please download the dataset from Piazza. There are about 65,000 ratings in total. We split the rating data into two set. You will train with 70% of the data (in *train_movie.csv*) and test on the remaining 30% of data (in *test_movie.csv*). Each of train and test files has lines having this format: UserID, MovieID, Rating. \n",
    "\n",
    "First you will need to load the data and store it with any structure you like. Please report the numbers of unique users and movies in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-910565b8a869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmovie_vec_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_vec_mlp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'concat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Concat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mconcat_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FullyConnected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "n_latent_factors_user = 8\n",
    "n_latent_factors_movie = 10\n",
    "n_latent_factors_mf = 3\n",
    "n_users, n_movies = 76353, 10000\n",
    "\n",
    "movie_input = keras.layers.Input(shape=[1],name='Item')\n",
    "movie_embedding_mlp = keras.layers.Embedding(n_movies + 1, n_latent_factors_movie, name='Song-Embedding-MLP')(movie_input)\n",
    "movie_vec_mlp = keras.layers.Flatten(name='FlattenSongs-MLP')(movie_embedding_mlp)\n",
    "movie_vec_mlp = keras.layers.Dropout(0.2)(movie_vec_mlp)\n",
    "\n",
    "movie_embedding_mf = keras.layers.Embedding(n_movies + 1, n_latent_factors_mf, name='Music-Embedding-MF')(movie_input)\n",
    "movie_vec_mf = keras.layers.Flatten(name='FlattenSongs-MF')(movie_embedding_mf)\n",
    "movie_vec_mf = keras.layers.Dropout(0.2)(movie_vec_mf)\n",
    "\n",
    "\n",
    "user_input = keras.layers.Input(shape=[1],name='User')\n",
    "user_vec_mlp = keras.layers.Flatten(name='FlattenUsers-MLP')(keras.layers.Embedding(n_users + 1, n_latent_factors_user,name='User-Embedding-MLP')(user_input))\n",
    "user_vec_mlp = keras.layers.Dropout(0.2)(user_vec_mlp)\n",
    "\n",
    "user_vec_mf = keras.layers.Flatten(name='FlattenUsers-MF')(keras.layers.Embedding(n_users + 1, n_latent_factors_mf,name='User-Embedding-MF')(user_input))\n",
    "user_vec_mf = keras.layers.Dropout(0.2)(user_vec_mf)\n",
    "\n",
    "\n",
    "concat = keras.layers.concatenate([movie_vec_mlp, user_vec_mlp],name='Concat')\n",
    "concat_dropout = keras.layers.Dropout(0.2)(concat)\n",
    "dense = keras.layers.Dense(200,name='FullyConnected')(concat_dropout)\n",
    "dense_batch = keras.layers.BatchNormalization(name='Batch')(dense)\n",
    "dropout_1 = keras.layers.Dropout(0.2,name='Dropout-1')(dense_batch)\n",
    "dense_2 = keras.layers.Dense(100,name='FullyConnected-1')(dropout_1)\n",
    "dense_batch_2 = keras.layers.BatchNormalization(name='Batch-2')(dense_2)\n",
    "\n",
    "\n",
    "dropout_2 = keras.layers.Dropout(0.2,name='Dropout-2')(dense_batch_2)\n",
    "dense_3 = keras.layers.Dense(50,name='FullyConnected-2')(dropout_2)\n",
    "dense_4 = keras.layers.Dense(20,name='FullyConnected-3', activation='relu')(dense_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in train set is  541\n",
      "Number of movie in train set is  1211\n",
      "Number of users in test set is  541\n",
      "Number of movie in test set is  1211\n"
     ]
    }
   ],
   "source": [
    "# load the data, then print out the number of\n",
    "# movies and users in each of train and test sets.\n",
    "# Your Code Here...\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def read_csv(filename):\n",
    "    df = pd.read_csv(filename, sep=',', header=None, skiprows=1)\n",
    "    df.columns = ['user','movie','rating']\n",
    "    return df\n",
    "\n",
    "train_df = read_csv('train_movie.csv')\n",
    "\n",
    "number_users = len(train_df.user.unique())\n",
    "number_movies = len(train_df.movie.unique())\n",
    "\n",
    "print(\"Number of users in train set is \" , number_users)\n",
    "print(\"Number of movie in train set is \" , number_movies)\n",
    "test_df = read_csv('test_movie.csv')\n",
    "print(\"Number of users in test set is \" , len(test_df.user.unique()))\n",
    "print(\"Number of movie in test set is \" , len(test_df.movie.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Implement User-based Collaborative Filtering\n",
    "\n",
    "In this part, you will implement the basic User–User Collaborative Filtering algorithm introduced in the class. Given the ratings by each user, you are going to try different methods in calculating the similarities between users. You will use equation `(c)` in `week06rec.pdf` (Page 40) to aggregate ratings. Set k = 0.05. Just consider all users as neighbors. That is, while predicting how user $u$ will rate item $i$, $\\widehat{C}$ includes all users who have ratings on i in the training set.\n",
    "\n",
    "*For this memory-based algorithm, you can only rely on the ratings in training set to predict for the testing set. That is, you assume that you don't know any ratings information in the test set except that when you evalaute your model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "First, you will try to calculate the similarity between users with cosine similary following the equation on page 16 of [Collaborative Filtering Recommender Systems](http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf). And then you need to predict the rating for each (user, movie) tuple in the test set. *Note: you don't need to subtract user mean baseline from the ratings prior to computing the similarity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "# Predict for test set\n",
    "avg_rating = train_df.groupby(['user'],as_index=False)['rating'].mean().values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "norm_score = [math.sqrt(sum(train_df[train_df[\"user\"] == i].rating ** 2)) for i in range(0, number_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_matrix = train_df.pivot_table(index='movie', columns='user', values='rating').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def append_lst(lst, m):\n",
    "    lst.append(m)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie = train_df.groupby(['user'],as_index=False)['movie'].apply(lambda movie: reduce(append_lst, movie, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_user = train_df.groupby(['movie'],as_index=False)['user'].apply(lambda user: reduce(append_lst, user, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_score(a,b):\n",
    "    score = [movie_matrix[a][i] * movie_matrix[b][i] for i in user_movie[a]]\n",
    "    return sum(score)/(norm_score[a]*norm_score[b])\n",
    "\n",
    "cosine_scores = [[0 for x in range(number_users)] for y in range(number_users)]\n",
    "\n",
    "for i in range(number_users):\n",
    "    for j in range(0,i):\n",
    "        cosine_scores[i][j] = cosine_score(i,j)\n",
    "        cosine_scores[j][i] = cosine_scores[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user, movie):\n",
    "    score = [cosine_scores[user][i] * (movie_matrix[i][movie] - avg_rating[i]) for i in range(0,number_users) if i in movie_user[movie]]\n",
    "    return 0.05 * sum(score) + avg_rating[user]\n",
    "    \n",
    "def predict_ratings():\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        actual_ratings.append(row[\"rating\"])\n",
    "        predicted_ratings.append(predict_rating(row[\"user\"], row[\"movie\"]))\n",
    "    return predicted_ratings, actual_ratings\n",
    "    \n",
    "predicted_ratings_cosine, actual_ratings = predict_ratings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "# Report Mean Absolute Error and Root Mean Squared Error for test set\n",
    "\n",
    "import math\n",
    "\n",
    "def RMS(actual, predicted):\n",
    "    ms = 0\n",
    "    for i in range(0,len(actual)):\n",
    "        ms = ms + math.pow((actual[i] - predicted[i]),2)\n",
    "    ms = ms / len(actual)\n",
    "    return math.sqrt(ms)\n",
    "\n",
    "def MAE(actual, predicted):\n",
    "    ms = 0\n",
    "    for i in range(0,len(actual)):\n",
    "        ms = ms + abs((actual[i] - predicted[i]))\n",
    "    return ms / len(actual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.8991489107420851\n",
      "MAE =  0.6903937198616398\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE = \", RMS(actual_ratings, predicted_ratings_cosine))\n",
    "print(\"MAE = \", MAE(actual_ratings, predicted_ratings_cosine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation\n",
    "\n",
    "Then, you will try to calculate the similarity between users with pearson correlation following `week06rec.pdf` (Page 37). And then you need to predict the rating for each (user, movie) tuple in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson(a, b):\n",
    "    common_movies = list(set(user_movie[a]) & set(user_movie[b]))\n",
    "    score = [(movie_matrix[a][i] - avg_rating[a]) * (movie_matrix[b][i] - avg_rating[b]) for i in common_movies]\n",
    "    d1 = [math.pow((movie_matrix[a][i] - avg_rating[a]),2) for i in common_movies]\n",
    "    d2 = [math.pow((movie_matrix[b][i] - avg_rating[b]),2) for i in common_movies]\n",
    "    if(sum(d1) != 0 and sum(d2) != 0):\n",
    "        return sum(score)/math.sqrt(sum(d1) * sum(d2))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "pearson_scores = [[0 for x in range(number_users)] for y in range(number_users)]\n",
    "\n",
    "for i in range(number_users):\n",
    "    for j in range(0,i):\n",
    "        pearson_scores[i][j] = pearson(i,j)\n",
    "        pearson_scores[j][i] = pearson_scores[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "# Predict for test set\n",
    "\n",
    "def predict_rating_pearson(user, movie):\n",
    "    score = [pearson_scores[user][i] * (movie_matrix[i][movie] - avg_rating[i]) for i in range(0,number_users) if i in movie_user[movie]]\n",
    "    return 0.05 * sum(score) + avg_rating[user]\n",
    "    \n",
    "def predict_ratings_pearson():\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        actual_ratings.append(row[\"rating\"])\n",
    "        predicted_ratings.append(predict_rating_pearson(row[\"user\"], row[\"movie\"]))\n",
    "    return predicted_ratings, actual_ratings\n",
    "    \n",
    "predicted_ratings_pearson, actual_ratings = predict_ratings_pearson()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.8983386235087089\n",
      "MAE =  0.6917506759527966\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here...\n",
    "# Report Mean Absolute Error and Root Mean Squared Error for test\n",
    "\n",
    "print(\"RMSE = \", RMS(actual_ratings, predicted_ratings_pearson))\n",
    "print(\"MAE = \", MAE(actual_ratings, predicted_ratings_pearson))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation (varying the threshold)\n",
    "\n",
    "In [Collaborative Filtering Recommender Systems](http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf), they observe that: \n",
    "\n",
    "> Pearson correlation suffers from computing high similarity\n",
    "between users with few ratings in common. This can be alleviated by setting a threshold on the number of co-rated items\n",
    "necessary for full agreement (correlation of 1) and scaling the\n",
    "similarity when the number of co-rated items falls below this\n",
    "threshold.\n",
    "\n",
    "So now revise your Pearson to consider a threshold. Try several values and report for one that you think is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "# Predict for test set\n",
    "\n",
    "# Your Code Here...\n",
    "# Predict for test set\n",
    "\n",
    "def pearson_threshold(a, b, k):\n",
    "    common_movies = list(set(user_movie[a]) & set(user_movie[b]))\n",
    "    if len(common_movies) <= k:\n",
    "        return 0\n",
    "    else:\n",
    "        return pearson_scores[a][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+\n",
      "| K  |  RMS  |  MAE  |\n",
      "+----+-------+-------+\n",
      "| 5  | 0.891 | 0.685 |\n",
      "+----+-------+-------+\n",
      "| 6  | 0.891 | 0.686 |\n",
      "+----+-------+-------+\n",
      "| 7  | 0.891 | 0.686 |\n",
      "+----+-------+-------+\n",
      "| 8  | 0.892 | 0.687 |\n",
      "+----+-------+-------+\n",
      "| 9  | 0.893 | 0.688 |\n",
      "+----+-------+-------+\n",
      "| 10 | 0.894 | 0.688 |\n",
      "+----+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "from beautifultable import BeautifulTable\n",
    "\n",
    "def predict_rating_pearson_threshold(user, movie, k):\n",
    "    score = [pearson_threshold(user, i, k) * (movie_matrix[i][movie] - avg_rating[i]) for i in range(0,number_users) if i in movie_user[movie]]\n",
    "    return 0.05 * sum(score) + avg_rating[user]\n",
    "    \n",
    "def predict_ratings_pearson_threshold(k):\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        actual_ratings.append(row[\"rating\"])\n",
    "        predicted_ratings.append(predict_rating_pearson_threshold(row[\"user\"], row[\"movie\"], k))\n",
    "    return predicted_ratings, actual_ratings\n",
    "    \n",
    "t = BeautifulTable()\n",
    "t.column_headers = [\"K\", \"RMS\", \"MAE\"]\n",
    "\n",
    "max_k = -1\n",
    "\n",
    "for k in range(5,11):\n",
    "    predicted_ratings_pearson_threshold, actual_ratings = predict_ratings_pearson_threshold(k)\n",
    "    t.append_row([k, RMS(actual_ratings, predicted_ratings_pearson_threshold), MAE(actual_ratings, predicted_ratings_pearson_threshold)])\n",
    "\n",
    "print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.8906289016916706\n",
      "MAE =  0.6852894156458275\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here...\n",
    "# Report Mean Absolute Error and Root Mean Squared Error for test\n",
    "# For k = 5 we obtained the lowest RMSE and MAE\n",
    "\n",
    "predicted_ratings_pearson_threshold, actual_ratings = predict_ratings_pearson_threshold(k=5)\n",
    "print(\"RMSE = \", RMS(actual_ratings, predicted_ratings_pearson_threshold))\n",
    "print(\"MAE = \", MAE(actual_ratings, predicted_ratings_pearson_threshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? How different are your results for the original Pearson Correlation approach vs. the thresholded version vs. the Cosine Similarity approach? Why do you think this is? *provide a brief (1-2 paragraph) discussion based on these questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the three measures work well in our case. Pearson correlation is simply the cosine similarity when you deduct the mean. Pearson correlation with the threshold version performs the best, then Pearon correlation appraoch(without any thresshhold) and then cosine similarity. The results of all the three measure are close enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. MF (20 points)\n",
    "\n",
    "In class, we introduced how matrix factorization works for recommendation. Now it is your term to implement it. There are different methods to obtain the latent factor matrices **P** and **Q**, like gradient descent, Alternating Least Squares (ALS), and so on. Pick one of them and implement your MF model. *You can refer to tutorials and resources online. Remember our **collaboration policy** and you need to inform us of the resources you refer to.* \n",
    "\n",
    "Please report MAE and RMSE of your MF model for the test set. *You are expected to get a lower MAE and RMSE compared with the results in Part 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "# Report Mean Absolute Error and Root Mean Squared Error for test\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def matrix_factorization(R, P, Q, K, steps=100, alpha=0.0003, beta=0.04):\n",
    "    Q = Q.T\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - np.dot(P[i,:],Q[:,j])\n",
    "                    for k in range(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "        eR = np.dot(P,Q)\n",
    "        e = 0\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e = e + pow(R[i][j] - np.dot(P[i,:],Q[:,j]), 2)\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * ( pow(P[i][k],2) + pow(Q[k][j],2) )\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T\n",
    "\n",
    "R = np.array(movie_matrix)\n",
    "N = len(R)\n",
    "M = len(R[0])\n",
    "K = 2\n",
    "\n",
    "P = np.random.rand(N,K)\n",
    "Q = np.random.rand(M,K)\n",
    "\n",
    "nP, nQ = matrix_factorization(R, P, Q, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nR = np.dot(nP, nQ.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings_MF():\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        actual_ratings.append(row[\"rating\"])\n",
    "        predicted_ratings.append(nR[row[\"movie\"]][row[\"user\"]])\n",
    "    return predicted_ratings, actual_ratings\n",
    "\n",
    "predicted_ratings_MF, actual_ratings_MF = predict_ratings_MF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.8874876773763827\n",
      "MAE = 0.6791439414090075\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE =\", RMS(actual_ratings_MF, predicted_ratings_MF))\n",
    "print(\"MAE =\", MAE(actual_ratings_MF, predicted_ratings_MF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which method did you use to obtain **P** and **Q**? What are the advantages and disadvantages of the method you pick? *provide a brief (1-2 paragraph) discussion based on these questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used gradient descent to find P and Q matrices. Also I used regularization to avoid overfitting. The biggest advantage of gradient descent is that is requires no computation of any second-derivatives (Hessian matrix), which makes it computationally fast per iteration. The disadvantage with gradient descent is that it takes lot of iterations to converge if $\\alpha$ (learning rate) is not set properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Extension (30 points)\n",
    "\n",
    "Given your results in the previous parts, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach. Hints: You may consider using the title or genres information, trying other algorithms, designing a hybrid system, and so on. *As in the last homework, you can do anything you like to improve MAE and RMSE.*\n",
    "\n",
    "You will get full marks for this part if you get better results than both of your CF and MF (of course we will also judge whether what you do here is reasonable or not). Additionally, you will get 5 points as bonus if your model performs the best among the whole class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here...\n",
    "# Report Mean Absolute Error and Root Mean Squared Error for test\n",
    "\n",
    "import surprise\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "\n",
    "reader = surprise.Reader(sep=',', skip_lines=1)\n",
    "data = surprise.Dataset.load_from_file('train_movie.csv', reader)\n",
    "\n",
    "alg = surprise.SVDpp()\n",
    "output = alg.fit(data.build_full_trainset())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings_SVDpp():\n",
    "    actual_ratings = []\n",
    "    predicted_ratings_svdpp = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        actual_ratings.append(row[\"rating\"])\n",
    "        predict = alg.predict(str(row[\"user\"]), str(row[\"movie\"]))\n",
    "        predicted_ratings_svdpp.append(predict.est)\n",
    "    return predicted_ratings_svdpp, actual_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings_SVDpp, actual_ratings_SVDpp = predict_ratings_SVDpp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.8726606369093305\n",
      "MAE = 0.6697734194898748\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE =\", RMS(actual_ratings_SVDpp, predicted_ratings_SVDpp))\n",
    "print(\"MAE =\", MAE(actual_ratings_SVDpp, predicted_ratings_SVDpp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please explain what you do to improve the recommendation in 1-2 paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used and extension of Singular Value Decompositin (SVD) known as SVD++ to predict the movie ratings of users on test data.\n",
    "\n",
    "SVD++ is a extension of SVD which uses implicit ratings to train. In the above case, the implicit ratings are the movie rating given by users. It is used as a collaborative filtering model. It is a matrix factorization technique which is done on the user-item ratings matrix. It is used to find the two matrices P and Q. To be more precise SVD is a probabilistic matrix factorization (PMF). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2019\n",
    "\n",
    "\n",
    "# Homework 2:   word2vec + SVM + Evaluation\n",
    "\n",
    "### 100 points [6% of your final grade]\n",
    "\n",
    "### Due: Tuesday, February 26, 2019 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Understand word2vec-like term embeddings,  explore real-world challenges with SVM-based classifiers, understand and implement several evaluation metrics.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw2.ipynb`. For example, my homework submission would be something like `555001234_hw2.ipynb`. Submit this notebook via eCampus (look for the homework 2 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Term embeddings + SVM (80 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "\n",
    "For this homework, we will still play with Yelp reviews from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge). As in Homework 1, you'll see that each line corresponds to a review on a particular business. Each review has a unique \"ID\" and the text content is in the \"review\" field. Additionally, this time, we also offer you the \"label\". If `label=1`, it means that this review is `Food-relevant`. If `label=0`, it means that this review is `Food-irrelevant`. Similarly, we have already done some basic preprocessing on the reviews, so you can just tokenize each review using whitespace.\n",
    "\n",
    "There are about 40,000 reviews in total, in which about 20,000 reviews are \"Food-irrelevant\". We split the review data into two sets. *review_train.json* is the training set. *review_test.json* is the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please load the dataset\n",
    "# Your code below\n",
    "\n",
    "import json\n",
    "\n",
    "def read_data(data_file):\n",
    "    data = []\n",
    "    with open(data_file) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "train_data = read_data('review_train.json')\n",
    "test_data = read_data('review_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-trained term embeddings\n",
    "\n",
    "To save your time, you can make use of  pre-trained term embeddings. In this homework, we are using one of the great pre-trained models from [GloVe](https://nlp.stanford.edu/projects/glove/) based on 2 billion tweets. GloVe is quite similar to word2vec. Unzip the *glove.6B.50d.txt.zip* file and run the code below. You will be able to load the term embeddings model, with which each word can be represented with a 50-dimension vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the pre-trained term embeddings\n",
    "import numpy as np\n",
    "\n",
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    model = {line.split()[0].decode('utf-8'): np.array(list(map(float, line.split()[1:])))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have a vector representation for each word. First, we use the simple (arithmetic) **mean** of these vectors of words in a review to represent the review. *Note: Just ignore those words which are not in the corpus of this pre-trained model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please figure out the vector representation for each review in the training data and testing data.\n",
    "# Your code below\n",
    "\n",
    "def get_vectors(data):\n",
    "    review_vector = []\n",
    "    review_labels = []\n",
    "    for entries in data:\n",
    "        review_vector.append(np.mean([model[w] for w in entries[\"review\"].split() if w in model] or [np.zeros(dim)], axis=0))\n",
    "        review_labels.append(entries[\"label\"])\n",
    "    return (review_vector, review_labels)\n",
    "\n",
    "train_review_vector, train_review_labels = get_vectors(train_data)\n",
    "test_review_vector, test_review_labels = get_vectors(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "With the vector representations you get for each review, please train an SVM model to predict whether a given review is food-relevant or not. **You do not need to implement any classifier from scratch. You may use scikit-learn's built-in capabilities.** You can only train your model with reviews in *review_train.json*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM model training\n",
    "# Your code here\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(train_review_vector, train_review_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to predict whether a given review is food-relevant or not. Please report the overall accuracy, precision and recall of your model on the **testing data**. You should **implement the functions for accuracy, precision, and recall**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "test_predict_labels = clf.predict(test_review_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(actual_labels, predicted_labels):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(predicted_labels)): \n",
    "        if actual_labels[i]==predicted_labels[i]==1:\n",
    "           TP += 1\n",
    "        if predicted_labels[i]==1 and actual_labels[i]!=predicted_labels[i]:\n",
    "           FP += 1\n",
    "        if actual_labels[i]==predicted_labels[i]==0:\n",
    "           TN += 1\n",
    "        if predicted_labels[i]==0 and actual_labels[i]!=predicted_labels[i]:\n",
    "           FN += 1\n",
    "        \n",
    "    print(\"Precision: \", TP /(TP + FP))\n",
    "    print(\"Recall: \", TP / (TP + FN))\n",
    "    print(\"Accuracy: \", (TP + TN)/(TP + FP + TN + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8721167425074533\n",
      "Recall:  0.9349032800672834\n",
      "Accuracy:  0.8991610738255034\n"
     ]
    }
   ],
   "source": [
    "measure_performance(test_review_labels, test_predict_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-based embeddings\n",
    "\n",
    "Instead of taking the mean of term embeddings, you can directly train a **doc2vec** model for paragraph or document embeddings. You can refer to the paper [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053v2.pdf) for more details. And in this homework, you can make use of the implementation in [gensim](https://radimrehurek.com/gensim/models/doc2vec.html).\n",
    "\n",
    "Now, you need to:\n",
    "* Train a doc2vec model based on all reviews you have (training + testing sets).\n",
    "* Use the embeddings from your doc2vec model to represent each review and train a new SVM model.\n",
    "* Report the overall accuracy, precision and recall of your model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a doc2vec\n",
    "# Your code here\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "train_corpus = []\n",
    "test_corpus = []\n",
    "\n",
    "def read_corpus(data):\n",
    "    for i,line in enumerate(data):\n",
    "        yield TaggedDocument(gensim.utils.simple_preprocess(line[\"review\"]), [line[\"id\"]])\n",
    "    \n",
    "train_corpus = list(read_corpus(train_data))\n",
    "test_corpus = list(read_corpus(test_data))\n",
    "model_doc2vec = Doc2Vec(vector_size=50, min_count=5, workers=4, epochs=40) \n",
    "model_doc2vec.build_vocab(train_corpus + test_corpus)\n",
    "model_doc2vec.train(train_corpus + test_corpus, total_examples=model_doc2vec.corpus_count, epochs=model_doc2vec.epochs)\n",
    "train_vector = [model_doc2vec.docvecs[i] for i in range(0,len(train_corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(gamma='auto')\n",
    "clf.fit(train_vector, train_review_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    To use LDA instead of SVM run the below code:\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    To use LDA instead of SVM run the below code:\n",
    "'''\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "#clf = LDA()\n",
    "#clf.fit(train_vector, train_review_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9372804816859006\n",
      "Recall:  0.9426408746846089\n",
      "Accuracy:  0.9399328859060403\n"
     ]
    }
   ],
   "source": [
    "#performance on SVM\n",
    "\n",
    "#test_vector = [model_doc2vec.infer_vector(entries[\"review\"].split()) for entries in test_data]\n",
    "test_vector = []\n",
    "for i in range(len(train_corpus), len(train_corpus) + len(test_corpus)):\n",
    "    test_vector.append(model_doc2vec.docvecs[i])\n",
    "\n",
    "test_labels = clf.predict(test_vector)\n",
    "measure_performance(test_review_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? How different are your results for the term-based average approach vs. the doc2vec approach? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that the doc2vec approach performs better than the term based average approach. The doc2vec approach provides a increase in accuracy by 5% which is significant.\n",
    "\n",
    "doc2vec is an extension of word2vec where embeddings are constructed from entire documents (rather than the individual words). Doc2vec is a better version of word2vec because it takes word order into account, generalizes to longer documents, and can learn from unlabelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you do better?\n",
    "\n",
    "Finally, see if you can do better than either the word- or doc- based embeddings approach for classification. You may explore new features, new classifiers, etc. Whatever you like. Just provide your code and a justification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the word based embedding approach where we took the arithmetic mean of the word vectors, I tried to exclude stop words in english for training and testing. I can see that there is a increase in performance, because we are eliminating the words which are redundant and do not add much information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Please figure out the vector representation for each review in the training data and testing data.\n",
    "# Your code below\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "train_review_vector = []\n",
    "test_review_vector = []\n",
    "train_review_labels = []\n",
    "test_review_labels = []\n",
    "\n",
    "def create_freq_pairs(data):\n",
    "    freq_list = {}\n",
    "    for entries in data:\n",
    "        freq_dict = defaultdict(lambda:0)\n",
    "        for word in entries[\"review\"].split():\n",
    "            freq_dict[word] += 1\n",
    "        freq_list[entries[\"id\"]] = freq_dict\n",
    "    return freq_list\n",
    "\n",
    "def get_modified_vectors(data):\n",
    "    review_vector = []\n",
    "    review_labels = []\n",
    "    for entries in data:\n",
    "        review_vector.append(np.mean([model[w] for w in entries[\"review\"].split() if w in model and w not in stop_words] or [np.zeros(dim)], axis=0))\n",
    "        review_labels.append(entries[\"label\"])\n",
    "    return (review_vector, review_labels)\n",
    "\n",
    "freq_list = create_freq_pairs(train_data)\n",
    "stop_words = set(stopwords.words('english')) \n",
    "train_review_vector, train_review_labels = get_modified_vectors(train_data)\n",
    "test_review_vector, test_review_labels = get_modified_vectors(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(train_review_vector, train_review_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Tried using LDA instead of SVM, but SVM gives better performance.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Tried using LDA instead of SVM, but SVM gives better performance.\n",
    "'''\n",
    "\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "#clf = LDA()\n",
    "#clf.fit(train_review_vector, train_review_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_labels = clf.predict(test_review_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8902380571976354\n",
      "Recall:  0.9372582001682086\n",
      "Accuracy:  0.9110738255033557\n"
     ]
    }
   ],
   "source": [
    "measure_performance(test_review_labels, test_predict_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: NDCG (20 points)\n",
    "\n",
    "You calculated the recall and precision in Part 1 and now you get a chance to implement NDCG. \n",
    "\n",
    "Assume that Amy searches for \"food-relevant\" reviews in the **testing set** on two search engines `A` and `B`. Since the ground-truth labels for the reviews are unknown to A and B, they need to make a prediction for each review and then return a ranked list of results based on their probabilities. The results from A are in *search_result_A.json*, and the results from B are in *search_result_B.json*. Each line contains the id of a review and its corresponding ranking.\n",
    "\n",
    "You can check their labels in *review_test.json* while calculating the NDCG scores. If a review is \"food-relevant\", the relevance score is 1. Otherwise, the relevance score is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDCG for search_result_A.json\n",
    "# Your code here\n",
    "\n",
    "import math\n",
    "import operator\n",
    "\n",
    "test_dict = {}\n",
    "for k in test_data:\n",
    "        test_dict[k[\"id\"]] = k[\"label\"]\n",
    "        \n",
    "def parse_search_result(search_data_file):\n",
    "    search_data = {}\n",
    "    with open(search_data_file) as f:\n",
    "        for line in f:\n",
    "            search_data[line[8:30]] = int(line[40:-2])\n",
    "    return search_data\n",
    "        \n",
    "def calculate_dcg(search_data):      \n",
    "    dcg = [test_dict[x]/math.log2(search_data[x]+1) for x in search_data]\n",
    "    return sum(dcg)\n",
    "\n",
    "def calculate_idcg(p):\n",
    "    idcg = [k/math.log2(i + 1) for i,k in enumerate(p,1)]\n",
    "    return sum(idcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9182854705835336\n"
     ]
    }
   ],
   "source": [
    "search_a_data = parse_search_result(\"search_result_A.json\")\n",
    "dcg = calculate_dcg(search_a_data)\n",
    "p = sorted(test_dict.values(), reverse=True)[:len(search_a_data)]\n",
    "idcg = calculate_idcg(p)\n",
    "print(dcg/idcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9858173908934676\n"
     ]
    }
   ],
   "source": [
    "# NDCG for search_result_B.json\n",
    "# Your code here\n",
    "\n",
    "search_b_data = parse_search_result(\"search_result_B.json\")\n",
    "dcg = calculate_dcg(search_b_data)\n",
    "p = sorted(test_dict.values(), reverse=True)[:len(search_b_data)]\n",
    "idcg = calculate_idcg(p)\n",
    "print(dcg/idcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
